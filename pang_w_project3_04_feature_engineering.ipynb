{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap\n",
    "\n",
    "We have our random forest model with tuned parameters. Let's do some feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import stackoverflow_helper as soh\n",
    "import dictionaries as look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_import = pd.read_csv('/Users/pang/repos/stack-overflow-survey/_data/output', index_col='Respondent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_big = raw_import['JobSat']\n",
    "X_big = raw_import.drop(columns='JobSat')\n",
    "X_train_big, X_test_final, y_train_big, y_test_final = train_test_split(\n",
    "    X_big, y_big, test_size=0.20, random_state=4444)\n",
    "\n",
    "y = y_train_big\n",
    "X = X_train_big\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=4444)\n",
    "df = pd.DataFrame(y_train).merge(X_train, on='Respondent')\n",
    "df_te = pd.DataFrame(y_test).merge(X_test, on='Respondent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And as a reminder, here are the baselines from previous notebooks.\n",
    "\n",
    "```\n",
    "STUF\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Amplify bad managers with those who report toxic work environment and lack of support from management and no input into the technology**\n",
    "\n",
    "Re-run model with old data using the parameters we selected as our baslines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'roc_auc': 0.7337516655985696,\n",
       " 'accuracy': 0.6859636616402828,\n",
       " 'precision': 0.683034117162073,\n",
       " 'recall': 0.9854286180424804,\n",
       " 'f1': 0.8064914307554816}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soh.test_model(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a feature that gets expoentially larger the more complaints the respondent has about their work environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'roc_auc': 0.7507526527411255,\n",
       " 'accuracy': 0.7008045204142996,\n",
       " 'precision': 0.695589040238921,\n",
       " 'recall': 0.9681100180639396,\n",
       " 'f1': 0.8131798293675748}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plus_tr = pd.DataFrame()\n",
    "plus_te = pd.DataFrame()\n",
    "\n",
    "plus_tr['f_bad_mgr_score'] = (4 - df['MgrIdiot']) ** ((df['WorkChallenge_Lack of support from management']\\\n",
    "                                                       + df['WorkChallenge_Toxic work environment']\\\n",
    "                                                       + df['PurchaseHow']\\\n",
    "                                                       + df['PurchaseWhat']\\\n",
    "                                                       + 1))\n",
    "plus_te['f_bad_mgr_score'] = (4 - df_te['MgrIdiot']) ** ((df_te['WorkChallenge_Lack of support from management']\\\n",
    "                                                          + df_te['WorkChallenge_Toxic work environment']\\\n",
    "                                                          + df_te['PurchaseHow']\\\n",
    "                                                          + df_te['PurchaseWhat']\\\n",
    "                                                          + 1))\n",
    "\n",
    "r_mgr = (X_train).merge(plus_tr, on='Respondent')\n",
    "r_mgr_te = (X_test).merge(plus_te, on='Respondent')\n",
    "\n",
    "soh.test_model(r_mgr, y_train, r_mgr_te, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is enough improvement to keep the changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Desire to work with technology**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a lot of data around technology respondants want to work with. What if the general desire to work with tech leads to higher JobSat?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tech = soh.get_sum_of_tech(r_mgr)\n",
    "mgr_tec = r_mgr.merge(new_tech, left_index=True, right_index=True)\n",
    "\n",
    "new_tech_t = soh.get_sum_of_tech(r_mgr_te)\n",
    "mgr_tec_t = r_mgr.merge(new_tech_t, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'roc_auc': 0.7521090911285088,\n",
       " 'accuracy': 0.7048826855672197,\n",
       " 'precision': 0.6986360661682872,\n",
       " 'recall': 0.9650266549669955,\n",
       " 'f1': 0.8112145195166175}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soh.test_model(r_mgr, y_train, r_mgr_te, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not much of an improvement so we will drop this line of questioning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Review feature importance**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from below, the new feature related to how upset they are at the company is a fairly important feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.feature_selection import SelectFromModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MgrIdiot</th>\n",
       "      <td>0.039189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f_bad_mgr_score</th>\n",
       "      <td>0.017661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WorkChallenge_Lack of support from management</th>\n",
       "      <td>0.013128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WorkChallenge_Toxic work environment</th>\n",
       "      <td>0.011304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PurchaseHow</th>\n",
       "      <td>0.009080</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      0\n",
       "MgrIdiot                                       0.039189\n",
       "f_bad_mgr_score                                0.017661\n",
       "WorkChallenge_Lack of support from management  0.013128\n",
       "WorkChallenge_Toxic work environment           0.011304\n",
       "PurchaseHow                                    0.009080"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = ExtraTreesClassifier(n_estimators=50)\n",
    "clf = clf.fit(mgr_tec, y_train)\n",
    "importance_scores = clf.feature_importances_  \n",
    "\n",
    "feature_importance = pd.DataFrame(importance_scores.reshape(1,372), columns = mgr_tec.columns).T\n",
    "sorted_feats = feature_importance.sort_values(by=0, ascending=False)\n",
    "sorted_feats.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some metrics showing performance of some other models reviewed. Not all were saved for this notebook, but I've summarized my analysis below:\n",
    "\n",
    "- Just MgrIdiot: I wanted to see how effective just using the `MgrIdiot` field would be. We actually don't lose a lot here in terms of ROC AUC. We also actually get pretty good recall.\n",
    "- All Fields: This is our baseline.\n",
    "- MgrId/f_bad: This is the same data as the first column, but with the engineered feature `f_bad_mgr_score`. \n",
    "- Work Challenge: Model with just `MgrIdiot`, `WorkChallenge_Lack of support from management` and `WorkChallenge_Toxic work environment` had on the model.\n",
    "- All Mgrfields: Same as above but with `PurchaseHow` and `PurchaseWhat`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Metrics | Just MgrIdiot | All Fields | MgrId/f_bad | Work Challenge | All Mgrfields | \n",
    "| ----: | :----: | :----: | :----: | :----: | :----: | \n",
    "| **ROC AUC** | 0.6816 |  0.7423 | 0.6991 | 0.7068 | 0.7066 | \n",
    "| **Accuracy** | 0.7158 | 0.6906 | 0.7141 | 0.7114 | 0.7141 | \n",
    "| **Precision** | 0.7196 | 0.6826 | 0.7234 | 0.7234 | 0.7234 | \n",
    "| **Recall** | 0.9338 | 0.9857 | 0.9240 | 0.9180 | 0.9129 | \n",
    "| **F1** | 0.8128 | 0.8049 | 0.8091 | 0.8101 | 0.8091 | "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dummy] *",
   "language": "python",
   "name": "conda-env-dummy-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
